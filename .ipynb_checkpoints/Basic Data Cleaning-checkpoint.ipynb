{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6845451a-867a-490f-b06c-c96a940376b1",
   "metadata": {},
   "source": [
    "# Basic Data Cleaning\n",
    "## Identify Columns The contain a single Value\n",
    "Column that have a single observaton or value are probably useless for modeling. These columns or predictors are reffered to as zero-variance predictors as if we measured the variance(average value from the mean), it would be zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4dca299-f887-41cf-a497-c3227d4ba34c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     238\n",
      "1     297\n",
      "2     927\n",
      "3     933\n",
      "4     179\n",
      "5     375\n",
      "6     820\n",
      "7     618\n",
      "8     561\n",
      "9      57\n",
      "10    577\n",
      "11     59\n",
      "12     73\n",
      "13    107\n",
      "14     53\n",
      "15     91\n",
      "16    893\n",
      "17    810\n",
      "18    170\n",
      "19     53\n",
      "20     68\n",
      "21      9\n",
      "22      1\n",
      "23     92\n",
      "24      9\n",
      "25      8\n",
      "26      9\n",
      "27    308\n",
      "28    447\n",
      "29    392\n",
      "30    107\n",
      "31     42\n",
      "32      4\n",
      "33     45\n",
      "34    141\n",
      "35    110\n",
      "36      3\n",
      "37    758\n",
      "38      9\n",
      "39      9\n",
      "40    388\n",
      "41    220\n",
      "42    644\n",
      "43    649\n",
      "44    499\n",
      "45      2\n",
      "46    937\n",
      "47    169\n",
      "48    286\n",
      "49      2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#first we will import the important libraries whihc is pandas and numpy\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
    "df = pd.read_csv(url, header=None)\n",
    "print(df.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bcd329-113f-4444-b586-ed0a2e8490c3",
   "metadata": {},
   "source": [
    "## Identifying the columns with low variance, or very few values\n",
    "Depending on the size and type of the data we can look for the low variance values as they can also deviate the accuracy of the machine learning model or can bring errors.\n",
    "There are two to three ways of doing it one is using numpy other is pandas and last is VarianceThreshold from scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "093606a8-1c0a-42b1-906a-a70b3a98a1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 0: 238 unique values, 25.4% unique\n",
      "Column 1: 297 unique values, 31.7% unique\n",
      "Column 2: 927 unique values, 98.9% unique\n",
      "Column 3: 933 unique values, 99.6% unique\n",
      "Column 4: 179 unique values, 19.1% unique\n",
      "Column 5: 375 unique values, 40.0% unique\n",
      "Column 6: 820 unique values, 87.5% unique\n",
      "Column 7: 618 unique values, 66.0% unique\n",
      "Column 8: 561 unique values, 59.9% unique\n",
      "Column 9: 57 unique values, 6.1% unique\n",
      "Column 10: 577 unique values, 61.6% unique\n",
      "Column 11: 59 unique values, 6.3% unique\n",
      "Column 12: 73 unique values, 7.8% unique\n",
      "Column 13: 107 unique values, 11.4% unique\n",
      "Column 14: 53 unique values, 5.7% unique\n",
      "Column 15: 91 unique values, 9.7% unique\n",
      "Column 16: 893 unique values, 95.3% unique\n",
      "Column 17: 810 unique values, 86.4% unique\n",
      "Column 18: 170 unique values, 18.1% unique\n",
      "Column 19: 53 unique values, 5.7% unique\n",
      "Column 20: 68 unique values, 7.3% unique\n",
      "Column 21: 9 unique values, 1.0% unique\n",
      "Column 22: 1 unique values, 0.1% unique\n",
      "Column 23: 92 unique values, 9.8% unique\n",
      "Column 24: 9 unique values, 1.0% unique\n",
      "Column 25: 8 unique values, 0.9% unique\n",
      "Column 26: 9 unique values, 1.0% unique\n",
      "Column 27: 308 unique values, 32.9% unique\n",
      "Column 28: 447 unique values, 47.7% unique\n",
      "Column 29: 392 unique values, 41.8% unique\n",
      "Column 30: 107 unique values, 11.4% unique\n",
      "Column 31: 42 unique values, 4.5% unique\n",
      "Column 32: 4 unique values, 0.4% unique\n",
      "Column 33: 45 unique values, 4.8% unique\n",
      "Column 34: 141 unique values, 15.0% unique\n",
      "Column 35: 110 unique values, 11.7% unique\n",
      "Column 36: 3 unique values, 0.3% unique\n",
      "Column 37: 758 unique values, 80.9% unique\n",
      "Column 38: 9 unique values, 1.0% unique\n",
      "Column 39: 9 unique values, 1.0% unique\n",
      "Column 40: 388 unique values, 41.4% unique\n",
      "Column 41: 220 unique values, 23.5% unique\n",
      "Column 42: 644 unique values, 68.7% unique\n",
      "Column 43: 649 unique values, 69.3% unique\n",
      "Column 44: 499 unique values, 53.3% unique\n",
      "Column 45: 2 unique values, 0.2% unique\n",
      "Column 46: 937 unique values, 100.0% unique\n",
      "Column 47: 169 unique values, 18.0% unique\n",
      "Column 48: 286 unique values, 30.5% unique\n",
      "Column 49: 2 unique values, 0.2% unique\n"
     ]
    }
   ],
   "source": [
    "total_rows = len(df)\n",
    "\n",
    "# Loop through each column\n",
    "for i in df.columns:\n",
    "    unique_vals = df[i].nunique()\n",
    "    percent_unique = (unique_vals / total_rows) * 100\n",
    "    print(f\"Column {i}: {unique_vals} unique values, {percent_unique:.1f}% unique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cf6d0bc-7e93-4e02-ae2c-b9373e1e4d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21,9,0.961\n",
      "22,1,0.107\n",
      "24,9,0.961\n",
      "25,8,0.854\n",
      "26,9,0.961\n",
      "32,4,0.427\n",
      "36,3,0.320\n",
      "38,9,0.961\n",
      "39,9,0.961\n",
      "45,2,0.213\n",
      "49,2,0.213\n"
     ]
    }
   ],
   "source": [
    "total_rows = len(df)\n",
    "# get the percentage of all rows\n",
    "for column in df.columns:\n",
    "    unique_values = df[column].nunique()\n",
    "    percent_unique = (unique_values/total_rows)*100\n",
    "    if percent_unique < 1:\n",
    "        print(f\"{column},{unique_values},{percent_unique:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae695913-9109-45a3-ae19-87baa6a3750a",
   "metadata": {},
   "source": [
    "## Deleting the columns with single variable or few varaibles.\n",
    "if we wanted to delete all 11 cloumns with unique values less than 1 percent of rows; the example below demonstrates this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed66c589-0b63-472b-a2d1-c17e2b79307f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(937, 50)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2362acc6-8433-4c10-bbfa-1be7453243d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 22, 24, 25, 26, 32, 36, 38, 39, 45, 49]\n",
      "(937, 39)\n"
     ]
    }
   ],
   "source": [
    "total_rows = len(df)\n",
    "counts = df.nunique()\n",
    "percentage_values = (counts/total_rows)*100\n",
    "to_del = [col for col, pct in percentage_values.items() if pct < 1]\n",
    "print(to_del)\n",
    "to_drop = df.drop(to_del,axis=1, inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad42378-eda7-4e56-b4c6-1a15285cd45f",
   "metadata": {},
   "source": [
    "## Identifying the Rows that contain Duplicate Data\n",
    "Rows with identical data are could be useless for the modeling process, if not dangerously misleading during model evaluation.\n",
    "The pandas fuction duplicated() will report whether a given row is duplicated or not. all rows are marked as wither aFalse to indicated that it is not a duplicate or true to indicate that it is a duplicate. If there are plicates, the first occurrence of the row is marked False(bu defalult), as we might expect. the example below checks for duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e88ab91a-b8ab-4b27-bf2c-26f771784ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "       0    1    2    3               4\n",
      "34   4.9  3.1  1.5  0.1     Iris-setosa\n",
      "37   4.9  3.1  1.5  0.1     Iris-setosa\n",
      "142  5.8  2.7  5.1  1.9  Iris-virginica\n",
      "(150, 5)\n",
      "(147, 5)\n"
     ]
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv'\n",
    "ds= pd.read_csv(url, header=None)\n",
    "dups= ds.duplicated()\n",
    "print(dups.any())\n",
    "print(ds[dups])\n",
    "#to delete all the rows that contains the duplicate data.\n",
    "print(ds.shape)\n",
    "ds.drop_duplicates(inplace = True)\n",
    "print(ds.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
